{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart disease prediction - Model evaluation using nested cross validation\n",
    "In this notebook my goal is to evaluate different types of classifiers to see which type performs the best in predicting heart disease in patients based on a number of attributes. To do this I will perform nested cross validation on the full dataset to evaluate the generalized performance of the models. The data comes from [Kaggle and UCI](https://www.kaggle.com/ronitf/heart-disease-uci). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/heart.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.366337</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>131.623762</td>\n",
       "      <td>246.264026</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>149.646865</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.399340</td>\n",
       "      <td>0.729373</td>\n",
       "      <td>2.313531</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.082101</td>\n",
       "      <td>0.466011</td>\n",
       "      <td>1.032052</td>\n",
       "      <td>17.538143</td>\n",
       "      <td>51.830751</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.525860</td>\n",
       "      <td>22.905161</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.022606</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
       "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
       "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
       "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
       "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
       "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
       "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
       "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
       "\n",
       "             thal      target  \n",
       "count  303.000000  303.000000  \n",
       "mean     2.313531    0.544554  \n",
       "std      0.612277    0.498835  \n",
       "min      0.000000    0.000000  \n",
       "25%      2.000000    0.000000  \n",
       "50%      2.000000    1.000000  \n",
       "75%      3.000000    1.000000  \n",
       "max      3.000000    1.000000  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns can be described as such:\n",
    "\n",
    "**age** - age in years  \n",
    "**sex** - (1 = male; 0 = female)  \n",
    "**cp** - chest pain type  \n",
    "**trestbps** - resting blood pressure (in mm Hg on admission to the hospital)  \n",
    "**chol** - serum cholestoral in mg/dl  \n",
    "**fbs** - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)  \n",
    "**restecg** - resting electrocardiographic results  \n",
    "**thalach** - maximum heart rate achieved  \n",
    "**exang** - exercise induced angina (1 = yes; 0 = no)  \n",
    "**oldpeak** - ST depression induced by exercise relative to rest  \n",
    "**slope** - the slope of the peak exercise ST segment  \n",
    "**ca** - number of major vessels (0-3) colored by flourosopy  \n",
    "**thal** - 3 = normal; 6 = fixed defect; 7 = reversable defect  \n",
    "**target** - 1 or 0 (Whether heart disease is present in patient or not)  \n",
    "\n",
    "As we can see in the report above, the target variable is balanced between positive and negative in the dataset. There are no null values, and for the categorical categories, we need to create dummy variables to remove the linear relation between them. First, let's split the data into X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "a9466f5132a7bee79e72cee613c90c06d86e56a0"
   },
   "outputs": [],
   "source": [
    "X = data.drop(\"target\", axis=1)\n",
    "y= data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data\n",
    "After loading the data we can create dummy features for each of the categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "e245f49a769cb31e0e23d49796345f245da5b48a"
   },
   "outputs": [],
   "source": [
    "ca_dummies = pd.get_dummies(X[\"ca\"], prefix=\"ca\")\n",
    "cp_dummies = pd.get_dummies(X[\"cp\"], prefix=\"cp\")\n",
    "rest_ecg_dummies = pd.get_dummies(X[\"restecg\"], prefix=\"rest_ecg\")\n",
    "slope_dummies = pd.get_dummies(X[\"slope\"], prefix=\"slope\")\n",
    "thal_dummies = pd.get_dummies(X[\"thal\"], prefix=\"thal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And drop the old columns and merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "cb3ee6a78e8d42f793d9205b922c75642be4912d"
   },
   "outputs": [],
   "source": [
    "X = X.drop([\"ca\", \"cp\", \"restecg\", \"slope\", \"thal\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "997d86a6e32c84c193c2458e2cd2256f9730baab"
   },
   "outputs": [],
   "source": [
    "X = pd.concat((X, ca_dummies, cp_dummies, rest_ecg_dummies, slope_dummies, thal_dummies), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a normalized version of the features, scaling all of them to 0-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "columns = X.columns\n",
    "X_norm = pd.DataFrame(scaler.fit_transform(X), columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "Let's now get into the model evaluation. I want to test four different classifiers: Random Forest, SVC, LogisticRegression, and RidgeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "fc96d2ba433788e9e2e24b632f48864841b8c6c8"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate each classifiers general performance on our data without the risk of overfitting on our test set, I will use nested crossvalidation. Normally, we use cross validation with GridSearchCV to find the best hyperparameters for our model. In nested cross validation, we will do such a grid search and find the best score on the validation set for an \"outer\" 80-20 cross validation split. For a better explanation, see [here](https://sebastianraschka.com/faq/docs/evaluate-a-model.html). \n",
    "\n",
    "Since the target variable is balanced but the dataset is small enough that for cross validation it may not be balanced, and most of the classifiers are not made to return probabilities (which makes roc_auc score not optimal), I will use balanced accuracy as the metric. Let's define a function which takes in a classifier, parameter, and data, and returns a generalized score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_score(estimator, params, X_train, y_train):\n",
    "    \n",
    "    inner_kf = KFold(n_splits=5, random_state=10, shuffle=True)\n",
    "    \n",
    "    outer_kf = KFold(n_splits=5, random_state = 20, shuffle=True)\n",
    "\n",
    "    clf = GridSearchCV(estimator = estimator, param_grid=params, cv=inner_kf)\n",
    "    \n",
    "    nested_score = cross_val_score(clf, X=X_train, y=y_train, scoring=\"balanced_accuracy\", cv = outer_kf)\n",
    "    \n",
    "    return nested_score.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "5c3b93f72d0549ab1575ef718a52a0befdb54cc7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.798908359502181"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForest\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "params = {'criterion': ['entropy', 'gini'],\n",
    "          'max_depth': np.arange(2,11, 2),\n",
    "          'min_samples_leaf': np.arange(1,40, 10),\n",
    "          'min_samples_split': np.arange(2, 100, 10),\n",
    "          'n_estimators': [5, 10, 20]}\n",
    "\n",
    "\n",
    "rf_score = generalize_score(rf, params, X, y)\n",
    "rf_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "ae6f67f9c91594f2594153ea3828cdc3746cf5a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6385995962677885"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "params = {'kernel': ['rbf'],\n",
    "          'C': np.logspace(-5,5, 10),\n",
    "          'gamma': np.logspace(-3,0, 3),}\n",
    "\n",
    "svc_score = generalize_score(svc, params, X, y)\n",
    "\n",
    "svc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the regularization for SVCs are affected by the feature scaling, let's see if we can get a better result with normalized features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8304219172216885"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC Normalized\n",
    "\n",
    "# SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "params = {'kernel': ['rbf'],\n",
    "          'C': np.logspace(-5,5, 10),\n",
    "          'gamma': np.logspace(-3,0, 3),}\n",
    "\n",
    "svc_score_norm = generalize_score(svc, params, X_norm, y)\n",
    "\n",
    "svc_score_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "993741c87d9e9104653021f8ef4df1218a2624f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8331354561194377"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Not normalized\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "params = {\n",
    "          'C': np.logspace(-10,10, 100),\n",
    "          'solver': [\"liblinear\"]\n",
    "          }\n",
    "\n",
    "lr_score = generalize_score(lr, params, X, y)\n",
    "lr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing here as for SVC, with normalized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8270748500588316"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Normalized\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "params = {\n",
    "          'C': np.logspace(-10,10, 100),\n",
    "          'solver': [\"liblinear\"]\n",
    "          }\n",
    "\n",
    "lr_score_norm = generalize_score(lr, params, X_norm, y)\n",
    "lr_score_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are actually a little bit worse this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.831789755460236"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RidgeClassifier\n",
    "\n",
    "rc = RidgeClassifier()\n",
    "\n",
    "params = {\n",
    "          'alpha': np.logspace(-10,10, 100),\n",
    "          'solver': [\"auto\"]\n",
    "          }\n",
    "\n",
    "rc_score = generalize_score(rc, params, X_norm, y)\n",
    "rc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.831789755460236"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RidgeClassifier Normalized\n",
    "\n",
    "\n",
    "rc = RidgeClassifier()\n",
    "\n",
    "params = {\n",
    "          'alpha': np.logspace(-10,10, 100),\n",
    "          'solver': [\"auto\"]\n",
    "          }\n",
    "\n",
    "rc_score_norm = generalize_score(rc, params, X_norm, y)\n",
    "rc_score_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & Conclusion\n",
    "Let's compared the classifiers by putting them into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Generalized accuracy score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.833135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge Classifier</td>\n",
       "      <td>0.831790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ridge Classifier Normalized</td>\n",
       "      <td>0.831790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC Normalized</td>\n",
       "      <td>0.830422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression Normalized</td>\n",
       "      <td>0.827075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.798908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.638600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Classifier  Generalized accuracy score\n",
       "3             Logistic Regression                    0.833135\n",
       "5                Ridge Classifier                    0.831790\n",
       "6     Ridge Classifier Normalized                    0.831790\n",
       "2                  SVC Normalized                    0.830422\n",
       "4  Logistic Regression Normalized                    0.827075\n",
       "0                   Random Forest                    0.798908\n",
       "1                             SVC                    0.638600"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Classifier\": [\"Random Forest\", \n",
    "                                       \"SVC\", \n",
    "                                       \"SVC Normalized\", \n",
    "                                       \"Logistic Regression\", \n",
    "                                       \"Logistic Regression Normalized\", \n",
    "                                       \"Ridge Classifier\", \n",
    "                                       \"Ridge Classifier Normalized\"],\n",
    "                       \"Generalized accuracy score\": [rf_score,\n",
    "                                                     svc_score,\n",
    "                                                     svc_score_norm,\n",
    "                                                     lr_score,\n",
    "                                                     lr_score_norm,\n",
    "                                                     rc_score,\n",
    "                                                     rc_score_norm]})\n",
    "    \n",
    "results.sort_values(\"Generalized accuracy score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the table above, Logistic Regression performs slightly better than the other algorithms, with the only outlier being SVC without normalized features. Let's fit a model using LR and look at the results. This time we will use cross validation and grid search to find the best hyperparameters. To validate our results, we can run 20 iterations of splitting into train and test and running cross validation to get the best parameters, and check the average balanced accuracy on the test set. This is similar to what we did above for all models, with the difference being that we are splitting into training and test randomly instead of using KFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "accuracys = []\n",
    "lr = LogisticRegression()\n",
    "\n",
    "params = {\n",
    "          'C': np.logspace(-10,10, 100),\n",
    "          'solver': [\"liblinear\"]\n",
    "          }\n",
    "for i in range(20):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    clf = GridSearchCV(lr, param_grid=params, scoring=\"balanced_accuracy\", cv=cv)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    best = clf.best_estimator_\n",
    "    y_pred = best.predict(X_te)\n",
    "\n",
    "    accuracy = balanced_accuracy_score(y_te, y_pred)\n",
    "    \n",
    "    accuracys.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500685594024592"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the average balanced accuracy we get is 85%, which is similar to what we got above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
